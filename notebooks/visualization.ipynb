{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_obs = 500000\n",
    "n_dim = 3\n",
    "K     = 15\n",
    "GPU_names = get_available_gpus()\n",
    "n_max_iters = 20\n",
    "seed = 800594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data(n_obs, n_dim, seed):\n",
    "    (X, Y) = make_classification(n_samples            = n_obs    , \n",
    "                                 n_features           = n_dim    ,\n",
    "                                 n_informative        = n_dim    ,\n",
    "                                 n_redundant          = 0        ,\n",
    "                                 n_classes            = 3        ,\n",
    "                                 n_clusters_per_class = 1        ,\n",
    "                                 shuffle              = True     ,\n",
    "                                 random_state         = seed      )\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribuited_fuzzy_C_means(X, K, GPU_names, initial_centers, n_max_iters, M = 2):\n",
    "    setup_ts = time.time()\n",
    "    number_of_gpus = len(GPU_names)\n",
    "    \n",
    "    X_list = np.split( X[ (X.shape[0] % number_of_gpus)  :, : ], number_of_gpus )\n",
    "    \n",
    "    partial_Mu_sum_list = []\n",
    "    partial_Mu_X_sum_list = []\n",
    "    \n",
    "    with tf.name_scope('global'):\n",
    "        with tf.device('/cpu:0'):\n",
    "            global_centroids = tf.Variable(initial_centers)\n",
    "            \n",
    "    for GPU_num in range(number_of_gpus):\n",
    "        GPU_name = GPU_names[GPU_num]\n",
    "        \n",
    "        (X_mat) = X_list.pop()\n",
    "        (N, M) = X_mat.shape\n",
    "  \n",
    "        with tf.name_scope('scope_' + str(GPU_num)):\n",
    "            with tf.device(GPU_name) :\n",
    "                ####\n",
    "                # In the coments we denote :\n",
    "                # => N = Number of Observations\n",
    "                # => M = Number of Dimensions\n",
    "                # => K = Number of Centers\n",
    "                ####\n",
    "                # Data for GPU GPU_num to Clusterize\n",
    "                X = tf.constant(X_mat)\n",
    "\n",
    "                # Reshapes rep_centroids and  rep_points to format N x K x M so that \n",
    "                # the 2 matrixes have the same size\n",
    "                rep_centroids = tf.reshape(tf.tile(global_centroids, [N, 1]), [N, K, M])\n",
    "                rep_points = tf.reshape(tf.tile(X, [1, K]), [N, K, M])\n",
    "\n",
    "                # Calculates sum_squares, a matrix of size N x K\n",
    "                # This matrix is just(X-Y)^2\n",
    "                dist_to_centers = tf.sqrt( tf.reduce_sum(tf.square(tf.subtract( rep_points, rep_centroids) ), \n",
    "                                                         reduction_indices = 2) )\n",
    "                \n",
    "                # Calculates cluster_membership, a matrix of size N x K\n",
    "                tmp = tf.pow(dist_to_centers, -2 / (M - 1))\n",
    "                cluster_membership_with_nan = tf.div( tf.transpose(tmp), tf.reduce_sum(tmp, 1))\n",
    "                \n",
    "                # Error treatment for when there are zeros in count_means_aux\n",
    "                cluster_membership = tf.where(tf.is_nan(cluster_membership_with_nan), tf.zeros_like(cluster_membership_with_nan), cluster_membership_with_nan);\n",
    "                \n",
    "                MU = tf.pow(cluster_membership, M)\n",
    "                \n",
    "                # Calculates auxiliar matrixes \n",
    "                # Mu_X_sum of size \n",
    "                Mu_X_sum = tf.matmul(MU, X)\n",
    "                Mu_sum = tf.reduce_sum(MU, 1)\n",
    "                \n",
    "                partial_Mu_sum_list.append( Mu_sum )\n",
    "                partial_Mu_X_sum_list.append( Mu_X_sum )\n",
    "                \n",
    "    with tf.name_scope('global') :\n",
    "        with tf.device('/cpu:0') :\n",
    "            global_Mu_sum = tf.add_n( partial_Mu_sum_list )\n",
    "            global_Mu_X_sum = tf.transpose(  tf.add_n(partial_Mu_X_sum_list) )\n",
    "            \n",
    "            new_centers = tf.transpose( tf.div(global_Mu_X_sum, global_Mu_sum) )\n",
    "            \n",
    "            update_centroid = tf.group( global_centroids.assign(new_centers) )\n",
    "        \n",
    "    setup_time = float( time.time() - setup_ts )\n",
    "    initialization_ts = time.time()\n",
    "    \n",
    "    sess = tf.Session( config = tf.ConfigProto( log_device_placement = True ) )\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    initialization_time = float( time.time() - initialization_ts ) \n",
    "    \n",
    "    cost_and_computation_dataframe = pd.DataFrame()\n",
    "    computation_time = 0.0\n",
    "    for i in range(n_max_iters):\n",
    "        aux_ts = time.time()\n",
    "        [result, _] = sess.run([global_centroids, update_centroid])\n",
    "        computation_time += float(time.time() - aux_ts)\n",
    "        \n",
    "    end_time_result_df = pd.DataFrame()\n",
    "    end_time_result_df = end_time_result_df.append({    'setup_time'          : setup_time         ,\n",
    "                                                        'initialization_time' : initialization_time,\n",
    "                                                        'computation_time'    : computation_time    },\n",
    "                                                        ignore_index = True)\n",
    "    \n",
    "    end_resut = {   'end_center'         : result            ,\n",
    "                    'init_center'        : initial_centers   ,\n",
    "                    'end_time_result_df' : end_time_result_df }\n",
    "    return end_resut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, Y) = make_data( n_obs = n_obs,\n",
    "                        n_dim = n_dim,\n",
    "                        seed  = seed  )\n",
    "initial_centers = X[5 : 5+K, :]\n",
    "\n",
    "plt.scatter(X[1:1000, 0], X[1:1000, 1], alpha = 0.8, c = Y[1:1000], marker = (5, 2))\n",
    "plt.scatter(initial_centers[:, 0], initial_centers[:, 1], alpha = 1, c = 'red', marker = (5, 3))\n",
    "plt.show()\n",
    "\n",
    "result = distribuited_fuzzy_C_means(X, K, GPU_names, initial_centers, n_max_iters)\n",
    "print('result', result)\n",
    "centers = result['end_center']\n",
    "\n",
    "plt.scatter(X[1:1000, 0], X[1:1000, 1], alpha = 0.6, c = Y, marker = (3, 1))\n",
    "plt.scatter(centers[:, 0], centers[:, 1], alpha = 1, c = 'red', marker = (5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribuited_k_means(X, K, GPU_names, initial_centers, n_max_iters):\n",
    "    setup_ts = time.time()\n",
    "    number_of_gpus = len(GPU_names)\n",
    "\n",
    "    X_list = np.split( X[ (X.shape[0] % number_of_gpus)  :, : ], number_of_gpus )\n",
    "    \n",
    "    partial_directions = []\n",
    "    partial_values = []\n",
    "    partial_results = []\n",
    "    #partial_cost = [] ## Commented for performance\n",
    "    \n",
    "    with tf.name_scope('global'):\n",
    "        with tf.device('/cpu:0'):\n",
    "            global_centroids = tf.Variable(initial_centers)\n",
    "            \n",
    "    for GPU_num in range(number_of_gpus):\n",
    "        GPU_name = GPU_names[GPU_num]\n",
    "        \n",
    "        (X_mat) = X_list.pop()\n",
    "        (N, M) = X_mat.shape\n",
    "  \n",
    "        with tf.name_scope('scope_' + str(GPU_num)):\n",
    "            with tf.device(GPU_name) :\n",
    "                ####\n",
    "                # In the coments we denote :\n",
    "                # => N = Number of Observations\n",
    "                # => M = Number of Dimensions\n",
    "                # => K = Number of Centers\n",
    "                ####\n",
    "                # Data for GPU GPU_num to Clusterize\n",
    "                X = tf.constant(X_mat)\n",
    "\n",
    "                # Reshapes rep_centroids and  rep_points to format N x K x M so that \n",
    "                # the 2 matrixes have the same size\n",
    "                rep_centroids = tf.reshape(tf.tile(global_centroids, [N, 1]), [N, K, M])\n",
    "                rep_points = tf.reshape(tf.tile(X, [1, K]), [N, K, M])\n",
    "\n",
    "                # Calculates sum_squares, a matrix of size N x K\n",
    "                # This matrix is not sqrt((X-Y)^2), it is just(X-Y)^2\n",
    "                # Since we need just the argmin(sqrt((X-Y)^2)) wich is equal to \n",
    "                # argmin((X-Y)^2), it would be a waste of computation\n",
    "                sum_squares = tf.reduce_sum(tf.square(tf.subtract( rep_points, rep_centroids) ), \n",
    "                                                reduction_indices = 2)\n",
    "\n",
    "                # Use argmin to select the lowest-distance point\n",
    "                # This gets a matrix of size N x 1\n",
    "                best_centroids = tf.argmin(sum_squares, axis = 1)\n",
    "\n",
    "                # This Sums vector X by the best_centroids indexes(assigned clusters)\n",
    "                # And returns a vector of size K x M\n",
    "                total_means_aux = tf.unsorted_segment_sum(X, best_centroids, K)\n",
    "\n",
    "                # This counts how many data points by best_centroids indexes(assigned clusters)\n",
    "                # And returns a vector of size K x M\n",
    "                count_means_aux = tf.unsorted_segment_sum(tf.ones_like(X), best_centroids, K)\n",
    "                \n",
    "                # Calculates the new Center for this GPU\n",
    "                # Returns a matrix of size K x M\n",
    "                means_with_nan = tf.div( total_means_aux, count_means_aux )\n",
    "\n",
    "                # Error treatment for when there are zeros in count_means_aux\n",
    "                means = tf.where(tf.is_nan(means_with_nan), tf.zeros_like(means_with_nan), means_with_nan);\n",
    "\n",
    "                # Cost Function, wihch would used for stopping criteria\n",
    "                #cost = tf.reduce_sum( tf.reduce_min(sum_squares, axis = 1) ) ## Commented for performance\n",
    "                #partial_cost.append(cost) ## Commented for performance\n",
    "                \n",
    "            with tf.device('/cpu:0'):\n",
    "                y_count = tf.bincount(tf.to_int32(best_centroids), maxlength = K, minlength = K)\n",
    "                y_count_float = tf.cast(y_count, dtype = tf.float64)\n",
    "                partial_mu =  tf.multiply( tf.transpose(means), y_count_float )\n",
    "\n",
    "                partial_directions.append( y_count_float )\n",
    "                partial_values.append( partial_mu )\n",
    "                \n",
    "    with tf.name_scope('global') :\n",
    "        with tf.device('/cpu:0') :\n",
    "            sum_direction = tf.add_n( partial_directions )\n",
    "            sum_mu = tf.add_n( partial_values )\n",
    "            #total_cost = tf.add_n( partial_cost )\n",
    "\n",
    "            rep_sum_direction = tf.reshape(tf.tile(sum_direction, [M]), [M, K])\n",
    "            new_centers = tf.transpose( tf.div(sum_mu, rep_sum_direction) )\n",
    "\n",
    "            update_centroid = tf.group( global_centroids.assign(new_centers) )\n",
    "        \n",
    "    setup_time = float( time.time() - setup_ts )\n",
    "    initialization_ts = time.time()\n",
    "    \n",
    "    sess = tf.Session( config = tf.ConfigProto( log_device_placement = True ) )\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    initialization_time = float( time.time() - initialization_ts ) \n",
    "    \n",
    "    computation_time = 0.0\n",
    "    for i in range(n_max_iters):\n",
    "        aux_ts = time.time()\n",
    "        [result, _] = sess.run([global_centroids, update_centroid])\n",
    "        computation_time += float(time.time() - aux_ts)\n",
    "    \n",
    "    end_resut = {   'end_center'          : result             ,\n",
    "                    'init_center'         : initial_centers    ,\n",
    "                    'setup_time'          : setup_time         ,\n",
    "                    'initialization_time' : initialization_time,\n",
    "                    'computation_time'    : computation_time   ,\n",
    "                    'n_iter'              : i\n",
    "                }\n",
    "\n",
    "    return end_resut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X, Y) = make_data( n_obs = n_obs,\n",
    "                        n_dim = n_dim,\n",
    "                        seed  = seed  )\n",
    "initial_centers = X[5 : 5+K, :]\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha = 0.6, c = Y, marker = (3, 1))\n",
    "plt.scatter(initial_centers[:, 0], initial_centers[:, 1], alpha = 1, c = 'red', marker = (5, 3))\n",
    "plt.show()\n",
    "\n",
    "result = distribuited_k_means(X, K, GPU_names, initial_centers, n_max_iters)\n",
    "\n",
    "centers = result['end_center']\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha = 0.6, c = Y, marker = (3, 1))\n",
    "plt.scatter(centers[:, 0], centers[:, 1], alpha = 1, c = 'red', marker = (5, 3))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
