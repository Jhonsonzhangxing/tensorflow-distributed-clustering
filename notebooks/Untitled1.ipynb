{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lopac/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import threading\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    print(local_device_protos)\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def make_data(filepath, n_obs, n_dim, seed, K):\n",
    "\n",
    "    try:\n",
    "        os.remove(filepath)\n",
    "    except:\n",
    "        print('file not found')\n",
    "    finally:\n",
    "        (X, Y) = make_classification(n_samples            = n_obs    , \n",
    "                                     n_features           = n_dim    ,\n",
    "                                     n_informative        = n_dim    ,\n",
    "                                     n_redundant          = 0        ,\n",
    "                                     n_classes            = K        ,\n",
    "                                     n_clusters_per_class = 1        ,\n",
    "                                     shuffle              = True     ,\n",
    "                                     class_sep            = 1.5      ,\n",
    "                                     random_state         = seed      )\n",
    "        \n",
    "        np.savez(filepath, X=X, Y=Y)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8439826486129689063\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11314364416\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 197756180581825395\n",
      "physical_device_desc: \"device: 0, name: Tesla K40m, pci bus id: 0000:04:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11314364416\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 2035865274393290917\n",
      "physical_device_desc: \"device: 1, name: Tesla K40m, pci bus id: 0000:05:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:2\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11314364416\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 10900420732246120775\n",
      "physical_device_desc: \"device: 2, name: Tesla K40m, pci bus id: 0000:08:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:3\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11312372122\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 7006096846743645924\n",
      "physical_device_desc: \"device: 3, name: Tesla K40m, pci bus id: 0000:09:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:4\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11314364416\n",
      "locality {\n",
      "  bus_id: 2\n",
      "}\n",
      "incarnation: 8937026262294060008\n",
      "physical_device_desc: \"device: 4, name: Tesla K40m, pci bus id: 0000:84:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:5\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11314364416\n",
      "locality {\n",
      "  bus_id: 2\n",
      "}\n",
      "incarnation: 12605022483762607084\n",
      "physical_device_desc: \"device: 5, name: Tesla K40m, pci bus id: 0000:85:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:6\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11314364416\n",
      "locality {\n",
      "  bus_id: 2\n",
      "}\n",
      "incarnation: 13164385666709140251\n",
      "physical_device_desc: \"device: 6, name: Tesla K40m, pci bus id: 0000:88:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:7\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11312372122\n",
      "locality {\n",
      "  bus_id: 2\n",
      "}\n",
      "incarnation: 10793522056767886617\n",
      "physical_device_desc: \"device: 7, name: Tesla K40m, pci bus id: 0000:89:00.0, compute capability: 3.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "n_obs = 200000000\n",
    "n_dim = 2\n",
    "K     = 3\n",
    "GPU_names = get_available_gpus()\n",
    "n_max_iters = 20\n",
    "seed = 800594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data('test-data.npz', n_obs, n_dim, seed, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('test-data.npz') as data:\n",
    "    data_X = data['X']\n",
    "    data_Y = data['Y']\n",
    "\n",
    "    \n",
    "maxsize = 2 * 1024 * 1024 * 1024\n",
    "size_of_each = data_X.shape[1] * data_X.dtype.itemsize\n",
    "\n",
    "initial_centers = data_X[0:K, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_placeholder = tf.placeholder(data_X.dtype, data_X.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data_placeholder)\n",
    "num_items = np.floor(maxsize / size_of_each)\n",
    "dataset = dataset.batch(num_items)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_kmeans(GPU_names, batch_data, max_iters, sess):\n",
    "    partial_directions = []\n",
    "    partial_values = []\n",
    "    partial_results = []\n",
    "\n",
    "    with tf.name_scope('global'):\n",
    "        with tf.device('/cpu:0'):\n",
    "            batch_data.set_shape((num_items, batch_data.get_shape()[1]))\n",
    "            print(batch_data)\n",
    "            parts = tf.split(batch_data, len(GPU_names), 0)\n",
    "            print(parts)\n",
    "            global_centroids = tf.Variable(initial_centers)\n",
    "    \n",
    "    for GPU_num in range(len(GPU_names)):\n",
    "        GPU_name = GPU_names[GPU_num]\n",
    "        \n",
    "        (X_mat) = parts[GPU_num]\n",
    "        (N, M) = X_mat.get_shape().as_list()\n",
    "    \n",
    "        with tf.name_scope('scope_' + str(GPU_num)):\n",
    "            with tf.device(GPU_name) :\n",
    "                ####\n",
    "                # In the coments we denote :\n",
    "                # => N = Number of Observations\n",
    "                # => M = Number of Dimensions\n",
    "                # => K = Number of Centers\n",
    "                ####\n",
    "                # Data for GPU GPU_num to Clusterize\n",
    "                X = tf.Variable(X_mat)\n",
    "\n",
    "                # Reshapes rep_centroids and rep_points to format N x K x M so that \n",
    "                # the 2 matrixes have the same size\n",
    "                rep_centroids = tf.reshape(tf.tile(global_centroids, [N, 1]), [N, K, M])\n",
    "                rep_points = tf.reshape(tf.tile(X, [1, K]), [N, K, M])\n",
    "\n",
    "                # Calculates sum_squares, a matrix of size N x K\n",
    "                # This matrix is not sqrt((X-Y)^2), it is just(X-Y)^2\n",
    "                # Since we need just the argmin(sqrt((X-Y)^2)) wich is equal to \n",
    "                # argmin((X-Y)^2), it would be a waste of computation\n",
    "                sum_squares = tf.reduce_sum(tf.square(tf.subtract( rep_points, rep_centroids) ), axis = 2)\n",
    "\n",
    "                # Use argmin to select the lowest-distance point\n",
    "                # This gets a matrix of size N x 1\n",
    "                best_centroids = tf.argmin(sum_squares, axis = 1)\n",
    "            \n",
    "                means = []\n",
    "                for c in range(K):\n",
    "                    means.append(\n",
    "                        tf.reduce_mean(\n",
    "                            tf.gather(X, tf.reshape(tf.where(tf.equal(best_centroids, c)), [1,-1])), axis=[1]))\n",
    "\n",
    "                new_centroids = tf.concat(means, 0)\n",
    "                # print('GPU: ', GPU_name)\n",
    "                # print('Initial centers ', initial_centers)\n",
    "                # print('New centroids ', new_centroids.eval())\n",
    "                \n",
    "            with tf.device('/cpu:0'):\n",
    "                y_count = tf.cast(\n",
    "                    tf.bincount(tf.to_int32(best_centroids), maxlength = K, minlength = K), dtype = tf.float64)\n",
    "            \n",
    "                partial_mu =  tf.multiply( tf.transpose(new_centroids), y_count )\n",
    "\n",
    "                partial_directions.append( y_count )\n",
    "                partial_values.append( partial_mu )\n",
    "                \n",
    "    \n",
    "    with tf.name_scope('global') :\n",
    "        with tf.device('/cpu:0') :\n",
    "            sum_direction = tf.add_n( partial_directions )\n",
    "            sum_mu = tf.add_n( partial_values )\n",
    "\n",
    "            rep_sum_direction = tf.reshape(tf.tile(sum_direction, [M]), [M, K])\n",
    "            new_centers = tf.transpose( tf.div(sum_mu, rep_sum_direction) )\n",
    "\n",
    "            update_centroid = tf.group( global_centroids.assign(new_centers) )\n",
    "      \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(n_max_iters):\n",
    "        [result, _] = [global_centroids, update_centroid]\n",
    "            \n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto( allow_soft_placement = True )\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session(config = config) as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={data_placeholder: data_X})\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            item = sess.run(next_element)\n",
    "#             item = sess.run(distributed_kmeans(GPU_names, next_element, n_max_iters, sess))\n",
    "            print(item)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
