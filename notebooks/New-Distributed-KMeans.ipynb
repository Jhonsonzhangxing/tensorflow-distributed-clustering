{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed K-Means Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports libs needed and created function to get available GPUs and to make training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import threading\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def make_data(filepath, n_obs, n_dim, seed, K):\n",
    "\n",
    "    try:\n",
    "        os.remove(filepath)\n",
    "    except:\n",
    "        print('file not found')\n",
    "    finally:\n",
    "        (X, Y) = make_classification(n_samples            = n_obs    , \n",
    "                                     n_features           = n_dim    ,\n",
    "                                     n_informative        = n_dim    ,\n",
    "                                     n_redundant          = 0        ,\n",
    "                                     n_classes            = K        ,\n",
    "                                     n_clusters_per_class = 1        ,\n",
    "                                     shuffle              = True     ,\n",
    "                                     class_sep            = 1.5      ,\n",
    "                                     random_state         = seed      )\n",
    "        \n",
    "        np.savez(filepath, X=X, Y=Y)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates a Tensorflow InteractiveSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An InteractiveSession is used to \"debug\" the code. It allows you to see what's happening. On production code, use Session instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining variables used throughout the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_obs = 200000000\n",
    "n_dim = 2\n",
    "K     = 3\n",
    "GPU_names = get_available_gpus()\n",
    "n_max_iters = 20\n",
    "seed = 800594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data('test-data.npz', n_obs, n_dim, seed, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('test-data.npz')\n",
    "\n",
    "data_X = data['X']\n",
    "data_Y = data['Y']\n",
    "initial_centers = data_X[0:K, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide dataset into X equal parts, X being the number of available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizes = [len(arg) for arg in np.split( data_X[ (data_X.shape[0] % len(GPU_names)) :, :], len(GPU_names))]\n",
    "\n",
    "partial_directions = []\n",
    "partial_values = []\n",
    "partial_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = n_dim * n_obs * data_X.dtype.itemsize\n",
    "max_size = 2*1024*1024*1024\n",
    "\n",
    "num_batches = np.ceil(data_size / max_size)\n",
    "\n",
    "batches = np.array_split(data_X, num_batches)\n",
    "\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates Tensorflow variable with the initial centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('global'):\n",
    "    with tf.device('/cpu:0'):\n",
    "        all_data = tf.placeholder(data_X.dtype, shape=(data_X.shape), name='all_data')\n",
    "        print(all_data)\n",
    "\n",
    "        parts = tf.split(tf.Variable(all_data), sizes, 0)\n",
    "        print(tf.Variable(parts[0]))\n",
    "        \n",
    "        global_centroids = tf.Variable(initial_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each GPU, calculate new local centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for GPU_num in range(len(GPU_names)):\n",
    "    GPU_name = GPU_names[GPU_num]\n",
    "    \n",
    "    (X_mat) = parts[GPU_num]\n",
    "    (N, M) = X_mat.get_shape().as_list()\n",
    "    \n",
    "    with tf.name_scope('scope_' + str(GPU_num)):\n",
    "        with tf.device(GPU_name) :\n",
    "            ####\n",
    "            # In the coments we denote :\n",
    "            # => N = Number of Observations\n",
    "            # => M = Number of Dimensions\n",
    "            # => K = Number of Centers\n",
    "            ####\n",
    "            \n",
    "            # Data for GPU GPU_num to Clusterize            \n",
    "            X = tf.Variable(X_mat)\n",
    "\n",
    "            # Reshapes rep_centroids and rep_points to format N x K x M so that \n",
    "            # the 2 matrixes have the same size\n",
    "            rep_centroids = tf.reshape(tf.tile(global_centroids, [N, 1]), [N, K, M])\n",
    "            rep_points = tf.reshape(tf.tile(X, [1, K]), [N, K, M])\n",
    "\n",
    "            # Calculates sum_squares, a matrix of size N x K\n",
    "            # This matrix is not sqrt((X-Y)^2), it is just(X-Y)^2\n",
    "            # Since we need just the argmin(sqrt((X-Y)^2)) wich is equal to \n",
    "            # argmin((X-Y)^2), it would be a waste of computation\n",
    "            sum_squares = tf.reduce_sum(tf.square(tf.subtract( rep_points, rep_centroids) ), axis = 2)\n",
    "\n",
    "            # Use argmin to select the lowest-distance point\n",
    "            # This gets a matrix of size N x 1\n",
    "            best_centroids = tf.argmin(sum_squares, axis = 1)\n",
    "            \n",
    "            means = []\n",
    "            for c in range(K):\n",
    "                means.append(\n",
    "                    tf.reduce_mean(\n",
    "                        tf.gather(X, tf.reshape(tf.where(tf.equal(best_centroids, c)), [1,-1])), axis=[1]))\n",
    "\n",
    "            new_centroids = tf.concat(means, 0)\n",
    "#             print('GPU: ', GPU_name)\n",
    "#             print('Initial centers ', initial_centers)\n",
    "#             print('New centroids ', new_centroids.eval())\n",
    "                \n",
    "        with tf.device('/cpu:0'):\n",
    "            y_count = tf.cast(\n",
    "                tf.bincount(tf.to_int32(best_centroids), maxlength = K, minlength = K), dtype = tf.float64)\n",
    "            \n",
    "            partial_mu =  tf.multiply( tf.transpose(new_centroids), y_count )\n",
    "\n",
    "            partial_directions.append( y_count )\n",
    "            partial_values.append( partial_mu )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After local GPU centroids have been calculated, create new global centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('global') :\n",
    "    with tf.device('/cpu:0') :\n",
    "        sum_direction = tf.add_n( partial_directions )\n",
    "        sum_mu = tf.add_n( partial_values )\n",
    "\n",
    "        rep_sum_direction = tf.reshape(tf.tile(sum_direction, [M]), [M, K])\n",
    "        new_centers = tf.transpose( tf.div(sum_mu, rep_sum_direction) )\n",
    "\n",
    "        update_centroid = tf.group( global_centroids.assign(new_centers) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto( log_device_placement = True, allow_soft_placement = True )\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(data_X.dtype, data_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(config = config) as sess:\n",
    "    sess.run(init, feed_dict={all_data: data_X})\n",
    "    \n",
    "    for i in range(n_max_iters):\n",
    "        [result, _] = sess.run([global_centroids, update_centroid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting graphs to evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(data_X[0:np.min([data_X.shape[0], 10000]), 0], \n",
    "            data_X[0:np.min([data_X.shape[0], 10000]), 1], \n",
    "            c = data_Y[0:np.min([data_X.shape[0], 10000])],\n",
    "            alpha = 0.8, marker = (5, 2))\n",
    "plt.scatter(initial_centers[:, 0], initial_centers[:, 1], alpha = 1, c = 'red', marker = (5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(data_X[0:np.min([data_X.shape[0], 10000]), 0], \n",
    "            data_X[0:np.min([data_X.shape[0], 10000]), 1], \n",
    "            c = data_Y[0:np.min([data_X.shape[0], 10000])],\n",
    "            alpha = 0.8, marker = (3, 1))\n",
    "plt.scatter(result[:, 0], result[:, 1], alpha = 1, c = 'blue', marker = (5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
